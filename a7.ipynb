{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with different optimizers, network layouts, and activation functions\n",
    "\n",
    "Drawing on the simple MNIST classifier from task 6.2, evaluate different optimizers, network layouts, and activation functions.  \n",
    "To this end, first learn how to visualize the training history of a Keras model:\n",
    "* [Using the `History` of a training](https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras) (not detailed enough)\n",
    "* [Visualizing loss and metrics during training](https://www.tensorflow.org/guide/keras/train_and_evaluate#visualizing_loss_and_metrics_during_training)\n",
    "* [Using the `TensorBoard` callback in Keras](https://www.tensorflow.org/tensorboard/scalars_and_keras)\n",
    "\n",
    "To see the detailed evolution of errors, you should consider a resolution as fine-grained as your mini-batches.\n",
    "\n",
    "## Task 7.1\n",
    "\n",
    "Consider the simple network below (with a single hidden layer of 64 neurons) and compare the learning curves of networks with ReLu, logistic and tanh activations.\n",
    "What happens if you use different activation functions also in the output layer?\n",
    "\n",
    "## Task 7.2\n",
    "\n",
    "Going back to the original ReLu-based network, compare [various optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers), i.e. SGD, RMSProp, Adam, ...\n",
    "How does their performance differ?\n",
    "\n",
    "## Task 7.3\n",
    "\n",
    "Compare various network layouts:\n",
    "* Try to adapt the number of hidden neurons, e.g. 32, 64, 128.\n",
    "* Try to add more hidden layers.\n",
    "* What is the best performance you can achieve (neglecting all regularization approaches for now)?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Load MNIST data set\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Neural Network with a single hidden layer\n",
    "from keras import models, layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(28 * 28,)))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "            # Loss function to minimize\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            # List of metrics to monitor\n",
    "            metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=100, epochs=3,\n",
    "                    validation_data=(x_val, y_val))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}