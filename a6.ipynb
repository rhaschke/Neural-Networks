{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming a neural network layer\n",
    "\n",
    "[Keras](https://keras.io) is a high-level deep-learning framework building on top of [TensorFlow](https://www.tensorflow.org). These frameworks follow the _symbol-to-symbol derivatives_ approach, i.e. automatically derive a computational graph to calculate derivatives. You just need to declare your inputs as TensorFlow variables and use TensorFlow operations on them to compute the forward pass.  \n",
    "\n",
    "## Task 6.1\n",
    "\n",
    "Work through the [Keras tutorial on custom layers](https://keras.io/guides/making_new_layers_and_models_via_subclassing) to learn how to create your own neural network layer.  \n",
    "Create a custom Keras layer that computes Gaussian basis functions, i.e. a layer that maps an input vector $\\mathbf x \\in \\mathbb R^n$ onto an output vector $\\mathbf y = f(\\mathbf x) \\in \\mathbb R^m$ as follows:\n",
    "\\begin{align}\n",
    "  f: \\mathbf x \\in \\mathbb R^n \\mapsto \\left[w_i \\exp\\left(-\\frac{\\|\\mathbf x - \\boldsymbol\\mu_i\\|^2}{\\sigma_i^2}\\right)\\right]_{i=1..m} \\in \\mathbb R^m\n",
    "\\end{align}\n",
    "\n",
    "Instead of projecting an input $\\mathbf x$ onto a weight vector $\\mathbf w$ as the standard neuron function $f(\\mathbf x) = \\sigma(\\mathbf w \\cdot \\mathbf x + b)$ does, the Gaussian basis function becomes active (with weight $w_i$) for all inputs $\\mathbf x$ close to a prototype $\\boldsymbol \\mu_i$. This activation quickly decays with increasing distance of $\\mathbf x$ to $\\boldsymbol \\mu_i$. The parameter $\\sigma_i$ controls the width of the Gaussian, i.e. the size of the active region.\n",
    "\n",
    "For efficient tensor-based operations you need to correctly _broadcast_ the tensors for the difference operation: TensorFlow will pass an input matrix of shape `(batch size, input dim)` for $\\mathbf X$, while you will have a matrix of centers $\\boldsymbol \\mu$ of shape `(input dim, #units)`. To correctly [broadcast](https://numpy.org/doc/stable/user/basics.broadcasting.html) them together, you will need Keras' [`expand_dims()`](https://www.tensorflow.org/api_docs/python/tf/keras/backend/expand_dims) function to extend $\\mathbf X$'s shape to `(batch size, input dim, 1)`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class RBFLayer(keras.layers.Layer):\n",
    "    # TODO: Add initializer parameters to init\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "        # TODO: Do something with the additional parameters\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "        # TODO: Initialize properties depending on `input_shape`\n",
    "\n",
    "    def call(self, X):\n",
    "        # TODO: Process input and return\n",
    "        return X\n",
    "\n",
    "\n",
    "class InitCentersRandom(keras.initializers.Initializer):\n",
    "    \"\"\" Initializer to initialize centers of RBF network from random samples of the data set.\"\"\"\n",
    "\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        # TODO: Generate and return initialization\n",
    "        pass\n",
    "\n",
    "\n",
    "### dummy example to compute diffs between data X and centers mu\n",
    "X = tf.ones((3, 5))  # input tensor X with batch dimension 3 and data dim N=5\n",
    "mu = tf.ones((5, 2))  # tensor mu with data dim N=5 and 2 units\n",
    "diffs = K.expand_dims(X) - mu  # diffs tensor: 3 x 5 x 2\n",
    "print(X.shape, mu.shape, diffs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6.2\n",
    "\n",
    "Compare the performance of such a Gaussian basis function layer with that of a standard [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer on the MNIST dataset.  \n",
    "Hint: Utilize existing tutorials on setting up your first MNIST MLP with Keras, e.g. https://www.tensorflow.org/guide/keras/train_and_evaluate.\n",
    "\n",
    "To achieve decent performance, you want to:\n",
    "- Initialize the centers $\\boldsymbol \\mu_i$ from random data samples $\\mathbf x$ (create a custom [initializer](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/Initializer))\n",
    "- Initialize $\\sigma_i$ to the typical in-class distance between data points.  \n",
    "  Use [`scipy.spatial.distance_matrix`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html) to compute this statistics on a random selection of your input data.  \n",
    "  (Doing it on the full dataset will probably exhaust your memory.)\n",
    "- Initialize $w_i = 1$\n",
    "\n",
    "Questions:\n",
    "- How many parameters each of those networks have?\n",
    "- Which network trains faster / easier?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
