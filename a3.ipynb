{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Regression attempts to fit a function model to training data. Let's create a data set from a function, here $f = 3x^3 - 2x$, and add some Gaussian noise with standard deviation `sigma`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data(X, f=lambda x: 3*x**3 - 2*x, sigma=0.1):\n",
    "    y = f(X) + np.random.normal(scale=sigma, size=X.shape)\n",
    "    return y\n",
    "\n",
    "X = np.linspace(-1,1,9)\n",
    "y = generate_data(X)\n",
    "\n",
    "X_test = np.linspace(-1, 1, 41)\n",
    "y_test = generate_data(X_test, sigma=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Polynomial function fitting\n",
    "\n",
    "Augment the following code snippet to actually compute the nonlinear regression given the first $1..M$ training data points and considering a polynomial model of degree $P \\in \\{0..8\\}$.  \n",
    "What do you expect as optimal parameter values $w_0$ to $w_3$ for a polynomial model of degree $P=3$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    def predict(self, X):  # apply prediction model to data X\n",
    "        return np.dot(X, self.w)\n",
    "\n",
    "    def fit(self, X, y):  # TODO: compute optimal weights for training data (X,y)\n",
    "        self.w = np.zeros(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "# @interact makes the following function (update) interactive, with sliders for M and P\n",
    "@interact(M=(1, len(X)), P=(0, 8))\n",
    "def update(M=len(X), P=3):\n",
    "    def design_matrix(X):  # compute polynomial features\n",
    "        return np.power(np.transpose([X]), range(0, P+1))\n",
    "    \n",
    "    # train regression model\n",
    "    reg = Regression()\n",
    "    reg.fit(design_matrix(X[0:M]), y[0:M])  # fit the first M training points only\n",
    "    print('w: ', reg.w)\n",
    "\n",
    "    plt.figure(42, figsize=(15,7))\n",
    "    plt.plot(X_test, y_test, '.-', label=\"test data\")  # plot ground truth of test data\n",
    "    plt.plot(X_test, reg.predict(design_matrix(X_test)), 'r-', label=\"prediction\")  # plot model prediction on test data\n",
    "    plt.scatter(X[0:M], y[0:M], marker='x', label=\"train data\") # plot training data\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Explain, what is learned in the following situations:\n",
    "\n",
    "1. $M=9$, $P=0$\n",
    "2. $M=4$, $P=3$\n",
    "3. $M=9$, $P=8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Ridge Regression\n",
    "\n",
    "To improve the smoothness of the learned function model, we can use Ridge regression, which adds a regularization term to our cost function:\n",
    "\n",
    "${\\cal L} = \\frac{1}{M} \\|\\hat{\\mathbf y} - \\mathbf y\\|^2 + \\lambda \\|\\mathbf w\\|^2$\n",
    "\n",
    "Show and verify step by step that the minimization of this loss functions yields the learning rule of Ridge regression:\n",
    "\n",
    "$\\mathbf w^* = (\\mathbf X^t \\mathbf X + \\lambda \\mathbf 1)^{-1} \\mathbf X^t \\mathbf y$\n",
    "\n",
    "_Hint_: See [_The Matrix Cookbook_ (Petersen and Pedersen, 2012)](http://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf), for derivatives (section 2.4), norms (section 10.3) and other identities of vectors and matrices.\n",
    "\n",
    "Extend your implementation to perform Ridge regression.  \n",
    "Introduce $\\lambda$ as a new parameter `alpha` to your `update` function and determine a suitable value for the case $P=8, M=9$ from task 2.  \n",
    "Explain what happens for very large and very small values of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: LASSO\n",
    "\n",
    "Instead of using the L2 norm, i.e. the _squared_ Euclidean norm $\\|\\mathbf w\\|_2^2 = \\sum w_i^2$, one can also use the L1 norm, i.e. $\\|\\mathbf w\\|_1 = \\sum |w_i|$, as a regularization term.  \n",
    "This results in LASSO regression (_least absolute shrinkage and selection operator_).\n",
    "\n",
    "1. Replace your `Regression` instance with the [`Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html?highlight=lasso#sklearn.linear_model.Lasso) regressor from [scikit-learn](https://scikit-learn.org):  \n",
    "   ```python\n",
    "   from sklearn.linear_model import Lasso\n",
    "   ```  \n",
    "   Compare the resulting weight vectors with those from Ridge regression. What do you observe?\n",
    "\n",
    "2. Determine and sketch the derivatives of both norms w.r.t. $\\mathbf w$.  \n",
    "   What are the advantages and disadvantages of both regularization terms?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}